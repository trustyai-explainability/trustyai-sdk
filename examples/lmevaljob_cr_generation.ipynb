{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMEvalJob Custom Resource Generation\n",
    "\n",
    "This notebook demonstrates how to create LMEvalJob Custom Resources (CRs) using the TrustyAI SDK in a Pythonic way. We'll cover:\n",
    "\n",
    "1. **Simple LMEvalJob creation** for basic model evaluation\n",
    "2. **Builder pattern** for more control and flexibility\n",
    "3. **Custom task cards** with preprocessing steps\n",
    "4. **Complex LLM-as-a-Judge** configurations\n",
    "5. **Generic utilities** for building custom evaluations\n",
    "\n",
    "## Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data models\n",
    "# Convenience imports (same as above, but from eval package)\n",
    "# from trustyai.providers.eval import LMEvalJobBuilder, create_simple_lmeval_job\n",
    "import json\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from trustyai.core.lmevaljob import (\n",
    "    Loader,\n",
    "    TaskCard,\n",
    ")\n",
    "\n",
    "# Kubernetes client for TrustyAI resources\n",
    "from trustyai.core.trustyai_kubernetes_client import TrustyAIKubernetesClient\n",
    "\n",
    "# Builder and utility functions\n",
    "from trustyai.providers.eval.utils import (\n",
    "    LMEvalJobBuilder,\n",
    "    create_copy_step,\n",
    "    create_filter_by_condition_step,\n",
    "    create_literal_eval_step,\n",
    "    create_llm_as_judge_metric,\n",
    "    create_mt_bench_template,\n",
    "    create_rating_task,\n",
    "    create_rename_splits_step,\n",
    "    create_rename_step,\n",
    "    create_task_card_json,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple LMEvalJob Creation\n",
    "\n",
    "The easiest way to create an LMEvalJob is using the `create_simple_lmeval_job()` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created simple LMEvalJob:\n",
      "   Name: simple-evaluation\n",
      "   Namespace: test\n",
      "   Model: google/flan-t5-small\n",
      "   Tasks: ['hellaswag', 'arc_easy', 'boolq']\n",
      "\n",
      "‚úÖ Created simple LMEvalJob with limit:\n",
      "   Name: simple-evaluation-limited\n",
      "   Model: google/flan-t5-small\n",
      "   Tasks: ['hellaswag', 'arc_easy']\n",
      "   Limit: 100\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "apiVersion: trustyai.opendatahub.io/v1alpha1\n",
       "kind: LMEvalJob\n",
       "metadata:\n",
       "  name: simple-evaluation\n",
       "  namespace: test\n",
       "spec:\n",
       "  model: hf\n",
       "  modelArgs:\n",
       "  - name: pretrained\n",
       "    value: google/flan-t5-small\n",
       "  taskList:\n",
       "    taskNames:\n",
       "    - hellaswag\n",
       "    - arc_easy\n",
       "    - boolq\n",
       "  logSamples: true\n",
       "  allowOnline: true\n",
       "  allowCodeExecution: true\n",
       "  limit: '1'\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a simple evaluation job using the new static method\n",
    "simple_job = LMEvalJobBuilder.simple(\n",
    "    name=\"simple-evaluation\",\n",
    "    model_name=\"google/flan-t5-small\",\n",
    "    tasks=[\"hellaswag\", \"arc_easy\", \"boolq\"],\n",
    "    namespace=\"test\",\n",
    "    limit=1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Created simple LMEvalJob:\")\n",
    "print(f\"   Name: {simple_job.metadata.name}\")\n",
    "print(f\"   Namespace: {simple_job.metadata.namespace}\")\n",
    "print(f\"   Model: {simple_job.spec.modelArgs[0].value}\")\n",
    "print(f\"   Tasks: {simple_job.spec.taskList.taskNames}\")\n",
    "print()\n",
    "\n",
    "# Create another simple job with a limit\n",
    "simple_job_with_limit = LMEvalJobBuilder.simple(\n",
    "    name=\"simple-evaluation-limited\",\n",
    "    model_name=\"google/flan-t5-small\",\n",
    "    tasks=[\"hellaswag\", \"arc_easy\"],\n",
    "    namespace=\"trustyai\",\n",
    "    limit=100\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Created simple LMEvalJob with limit:\")\n",
    "print(f\"   Name: {simple_job_with_limit.metadata.name}\")\n",
    "print(f\"   Model: {simple_job_with_limit.spec.modelArgs[0].value}\")\n",
    "print(f\"   Tasks: {simple_job_with_limit.spec.taskList.taskNames}\")\n",
    "print(f\"   Limit: {simple_job_with_limit.spec.limit}\")\n",
    "print()\n",
    "\n",
    "# Display the YAML output\n",
    "display(Markdown(f\"```yaml\\n{simple_job.to_yaml()}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Builder Pattern for Advanced Configuration\n",
    "\n",
    "For more control over the job configuration, use the `LMEvalJobBuilder` class with its fluent API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created advanced LMEvalJob with builder pattern:\n",
      "   Name: advanced-eval\n",
      "   Model: microsoft/DialoGPT-medium\n",
      "   Limit: 50\n",
      "   Log samples: False\n",
      "   Environment variables: 3\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"apiVersion\": \"trustyai.opendatahub.io/v1alpha1\",\n",
       "  \"kind\": \"LMEvalJob\",\n",
       "  \"metadata\": {\n",
       "    \"name\": \"advanced-eval\",\n",
       "    \"namespace\": \"my-namespace\"\n",
       "  },\n",
       "  \"spec\": {\n",
       "    \"model\": \"hf\",\n",
       "    \"modelArgs\": [\n",
       "      {\n",
       "        \"name\": \"pretrained\",\n",
       "        \"value\": \"microsoft/DialoGPT-medium\"\n",
       "      }\n",
       "    ],\n",
       "    \"taskList\": {\n",
       "      \"taskNames\": [\n",
       "        \"piqa\",\n",
       "        \"winogrande\"\n",
       "      ]\n",
       "    },\n",
       "    \"logSamples\": false,\n",
       "    \"allowOnline\": true,\n",
       "    \"allowCodeExecution\": false,\n",
       "    \"limit\": \"50\",\n",
       "    \"pod\": {\n",
       "      \"container\": {\n",
       "        \"env\": [\n",
       "          {\n",
       "            \"name\": \"HF_TOKEN\",\n",
       "            \"value\": \"hf_your_token_here\"\n",
       "          },\n",
       "          {\n",
       "            \"name\": \"CUDA_VISIBLE_DEVICES\",\n",
       "            \"value\": \"0,1\"\n",
       "          },\n",
       "          {\n",
       "            \"name\": \"TRANSFORMERS_CACHE\",\n",
       "            \"value\": \"/tmp/cache\"\n",
       "          }\n",
       "        ]\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build a more advanced job using the builder pattern\n",
    "advanced_job = (LMEvalJobBuilder(\"advanced-eval\")\n",
    "                .namespace(\"my-namespace\")\n",
    "                .model(\"hf\")\n",
    "                .pretrained_model(\"microsoft/DialoGPT-medium\")\n",
    "                .task_names([\"piqa\", \"winogrande\"])\n",
    "                .limit(50)\n",
    "                .log_samples(False)\n",
    "                .allow_online(True)\n",
    "                .allow_code_execution(False)\n",
    "                .hf_token(\"hf_your_token_here\")\n",
    "                .env_var(\"CUDA_VISIBLE_DEVICES\", \"0,1\")\n",
    "                .env_var(\"TRANSFORMERS_CACHE\", \"/tmp/cache\")\n",
    "                .build())\n",
    "\n",
    "print(\"‚úÖ Created advanced LMEvalJob with builder pattern:\")\n",
    "print(f\"   Name: {advanced_job.metadata.name}\")\n",
    "print(f\"   Model: {advanced_job.spec.modelArgs[0].value}\")\n",
    "print(f\"   Limit: {advanced_job.spec.limit}\")\n",
    "print(f\"   Log samples: {advanced_job.spec.logSamples}\")\n",
    "print(f\"   Environment variables: {len(advanced_job.spec.pod.container.env)}\")\n",
    "print()\n",
    "\n",
    "# Display as JSON for variety\n",
    "display(Markdown(f\"```json\\n{advanced_job.to_json()}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Task Cards with Preprocessing Steps\n",
    "\n",
    "Create custom task cards using dataclasses and generic preprocessing step builders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created custom TaskCard:\n",
      "   Loader path: my-organization/custom-dataset\n",
      "   Split: validation\n",
      "   Preprocessing steps: 5\n",
      "   Task: custom.evaluation.task\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"__type__\": \"task_card\",\n",
       "  \"loader\": {\n",
       "    \"__type__\": \"load_hf\",\n",
       "    \"path\": \"my-organization/custom-dataset\",\n",
       "    \"split\": \"validation\"\n",
       "  },\n",
       "  \"preprocess_steps\": [\n",
       "    {\n",
       "      \"__type__\": \"rename_splits\",\n",
       "      \"mapper\": {\n",
       "        \"validation\": \"test\"\n",
       "      }\n",
       "    },\n",
       "    {\n",
       "      \"__type__\": \"filter_by_condition\",\n",
       "      \"values\": {\n",
       "        \"quality\": \"high\"\n",
       "      },\n",
       "      \"condition\": \"eq\"\n",
       "    },\n",
       "    {\n",
       "      \"__type__\": \"rename\",\n",
       "      \"field_to_field\": {\n",
       "        \"input_text\": \"question\",\n",
       "        \"target\": \"answer\"\n",
       "      }\n",
       "    },\n",
       "    {\n",
       "      \"__type__\": \"literal_eval\",\n",
       "      \"field\": \"metadata\"\n",
       "    },\n",
       "    {\n",
       "      \"__type__\": \"copy\",\n",
       "      \"field\": \"metadata/category\",\n",
       "      \"to_field\": \"category\"\n",
       "    }\n",
       "  ],\n",
       "  \"task\": \"custom.evaluation.task\",\n",
       "  \"templates\": [\n",
       "    \"custom.template\"\n",
       "  ]\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a custom loader\n",
    "custom_loader = Loader(\n",
    "    __type__=\"load_hf\",\n",
    "    path=\"my-organization/custom-dataset\",\n",
    "    split=\"validation\"\n",
    ")\n",
    "\n",
    "# Create preprocessing steps using helper functions\n",
    "preprocess_steps = [\n",
    "    create_rename_splits_step({\"validation\": \"test\"}),\n",
    "    create_filter_by_condition_step({\"quality\": \"high\"}, \"eq\"),\n",
    "    create_rename_step({\"input_text\": \"question\", \"target\": \"answer\"}),\n",
    "    create_literal_eval_step(\"metadata\"),\n",
    "    create_copy_step(\"metadata/category\", \"category\"),\n",
    "]\n",
    "\n",
    "# Create the task card with dataclasses\n",
    "custom_task_card = TaskCard(\n",
    "    __type__=\"task_card\",\n",
    "    loader=custom_loader,\n",
    "    preprocess_steps=preprocess_steps,\n",
    "    task=\"custom.evaluation.task\",\n",
    "    templates=[\"custom.template\"]\n",
    ")\n",
    "\n",
    "# Convert to JSON string\n",
    "card_json = create_task_card_json(custom_task_card)\n",
    "\n",
    "print(\"‚úÖ Created custom TaskCard:\")\n",
    "print(f\"   Loader path: {custom_loader.path}\")\n",
    "print(f\"   Split: {custom_loader.split}\")\n",
    "print(f\"   Preprocessing steps: {len(preprocess_steps)}\")\n",
    "print(f\"   Task: {custom_task_card.task}\")\n",
    "print()\n",
    "\n",
    "# Display the JSON structure\n",
    "card_data = json.loads(card_json)\n",
    "display(Markdown(f\"```json\\n{json.dumps(card_data, indent=2)}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created LMEvalJob with custom task card:\n",
      "   Job name: custom-card-eval\n",
      "   Has custom card: True\n",
      "   Card contains: 5 fields\n"
     ]
    }
   ],
   "source": [
    "# Use the custom task card in an LMEvalJob\n",
    "custom_card_job = (LMEvalJobBuilder(\"custom-card-eval\")\n",
    "                   .namespace(\"custom-ns\")\n",
    "                   .pretrained_model(\"my-custom-model\")\n",
    "                   .custom_card(\n",
    "                       card_json=card_json,\n",
    "                       template_ref=\"custom.template\",\n",
    "                       format_str=\"custom.format\",\n",
    "                       metrics=[\"custom_metric\"]\n",
    "                   )\n",
    "                   .build())\n",
    "\n",
    "print(\"‚úÖ Created LMEvalJob with custom task card:\")\n",
    "print(f\"   Job name: {custom_card_job.metadata.name}\")\n",
    "print(f\"   Has custom card: {custom_card_job.spec.taskList.taskRecipes is not None}\")\n",
    "print(f\"   Card contains: {len(json.loads(custom_card_job.spec.taskList.taskRecipes[0].card.custom))} fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MT-Bench Style LLM-as-a-Judge Configuration\n",
    "\n",
    "Create a complex LLM-as-a-Judge setup similar to the MT-Bench evaluation, demonstrating custom templates, tasks, and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created MT-Bench components:\n",
      "   Template: response_assessment.rating.mt_bench_single_turn\n",
      "   Task: response_assessment.rating.single_turn\n",
      "   Metric: llmaaj_metric\n",
      "   Task card preprocessing steps: 8\n"
     ]
    }
   ],
   "source": [
    "# Helper function to create MT-Bench style task card (like in our tests)\n",
    "def create_mt_bench_task_card(dataset_path: str = \"OfirArviv/mt_bench_single_score_gpt4_judgement\") -> str:\n",
    "    \"\"\"Create MT-Bench specific task card configuration.\"\"\"\n",
    "    loader = Loader(__type__=\"load_hf\", path=dataset_path, split=\"train\")\n",
    "\n",
    "    preprocess_steps = [\n",
    "        create_rename_splits_step({\"train\": \"test\"}),\n",
    "        create_filter_by_condition_step({\"turn\": 1}, \"eq\"),\n",
    "        create_filter_by_condition_step({\"reference\": \"[]\"}, \"eq\"),\n",
    "        create_rename_step({\n",
    "            \"model_input\": \"question\",\n",
    "            \"score\": \"rating\",\n",
    "            \"category\": \"group\",\n",
    "            \"model_output\": \"answer\"\n",
    "        }),\n",
    "        create_literal_eval_step(\"question\"),\n",
    "        create_copy_step(\"question/0\", \"question\"),\n",
    "        create_literal_eval_step(\"answer\"),\n",
    "        create_copy_step(\"answer/0\", \"answer\"),\n",
    "    ]\n",
    "\n",
    "    task_card = TaskCard(\n",
    "        __type__=\"task_card\",\n",
    "        loader=loader,\n",
    "        preprocess_steps=preprocess_steps,\n",
    "        task=\"tasks.response_assessment.rating.single_turn\",\n",
    "        templates=[\"templates.response_assessment.rating.mt_bench_single_turn\"]\n",
    "    )\n",
    "\n",
    "    return create_task_card_json(task_card)\n",
    "\n",
    "# Create the MT-Bench task card\n",
    "mt_bench_card_json = create_mt_bench_task_card()\n",
    "\n",
    "# Create custom definitions for LLM-as-a-Judge\n",
    "mt_bench_template = create_mt_bench_template()\n",
    "rating_task = create_rating_task()\n",
    "llm_judge_metric = create_llm_as_judge_metric(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "print(\"‚úÖ Created MT-Bench components:\")\n",
    "print(f\"   Template: {mt_bench_template.name}\")\n",
    "print(f\"   Task: {rating_task.name}\")\n",
    "print(f\"   Metric: {llm_judge_metric.name}\")\n",
    "print(f\"   Task card preprocessing steps: {len(json.loads(mt_bench_card_json)['preprocess_steps'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created complete LLM-as-a-Judge evaluation:\n",
      "   Job name: custom-llmaaj-metric\n",
      "   Model: google/flan-t5-small\n",
      "   Custom templates: 1\n",
      "   Custom tasks: 1\n",
      "   Custom metrics: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Complete LLM-as-a-Judge YAML:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "apiVersion: trustyai.opendatahub.io/v1alpha1\n",
       "kind: LMEvalJob\n",
       "metadata:\n",
       "  name: custom-llmaaj-metric\n",
       "spec:\n",
       "  model: hf\n",
       "  modelArgs:\n",
       "  - name: pretrained\n",
       "    value: google/flan-t5-small\n",
       "  taskList:\n",
       "    taskRecipes:\n",
       "    - card:\n",
       "        custom: \"{\\n    \\\"__type__\\\": \\\"task_card\\\",\\n    \\\"loader\\\": {\\n        \\\"\\\n",
       "          __type__\\\": \\\"load_hf\\\",\\n        \\\"path\\\": \\\"OfirArviv/mt_bench_single_score_gpt4_judgement\\\"\\\n",
       "          ,\\n        \\\"split\\\": \\\"train\\\"\\n    },\\n    \\\"preprocess_steps\\\": [\\n \\\n",
       "          \\       {\\n            \\\"__type__\\\": \\\"rename_splits\\\",\\n            \\\"\\\n",
       "          mapper\\\": {\\n                \\\"train\\\": \\\"test\\\"\\n            }\\n      \\\n",
       "          \\  },\\n        {\\n            \\\"__type__\\\": \\\"filter_by_condition\\\",\\n \\\n",
       "          \\           \\\"values\\\": {\\n                \\\"turn\\\": 1\\n            },\\n\\\n",
       "          \\            \\\"condition\\\": \\\"eq\\\"\\n        },\\n        {\\n            \\\"\\\n",
       "          __type__\\\": \\\"filter_by_condition\\\",\\n            \\\"values\\\": {\\n      \\\n",
       "          \\          \\\"reference\\\": \\\"[]\\\"\\n            },\\n            \\\"condition\\\"\\\n",
       "          : \\\"eq\\\"\\n        },\\n        {\\n            \\\"__type__\\\": \\\"rename\\\",\\n\\\n",
       "          \\            \\\"field_to_field\\\": {\\n                \\\"model_input\\\": \\\"\\\n",
       "          question\\\",\\n                \\\"score\\\": \\\"rating\\\",\\n                \\\"\\\n",
       "          category\\\": \\\"group\\\",\\n                \\\"model_output\\\": \\\"answer\\\"\\n \\\n",
       "          \\           }\\n        },\\n        {\\n            \\\"__type__\\\": \\\"literal_eval\\\"\\\n",
       "          ,\\n            \\\"field\\\": \\\"question\\\"\\n        },\\n        {\\n        \\\n",
       "          \\    \\\"__type__\\\": \\\"copy\\\",\\n            \\\"field\\\": \\\"question/0\\\",\\n \\\n",
       "          \\           \\\"to_field\\\": \\\"question\\\"\\n        },\\n        {\\n        \\\n",
       "          \\    \\\"__type__\\\": \\\"literal_eval\\\",\\n            \\\"field\\\": \\\"answer\\\"\\n\\\n",
       "          \\        },\\n        {\\n            \\\"__type__\\\": \\\"copy\\\",\\n          \\\n",
       "          \\  \\\"field\\\": \\\"answer/0\\\",\\n            \\\"to_field\\\": \\\"answer\\\"\\n    \\\n",
       "          \\    }\\n    ],\\n    \\\"task\\\": \\\"tasks.response_assessment.rating.single_turn\\\"\\\n",
       "          ,\\n    \\\"templates\\\": [\\n        \\\"templates.response_assessment.rating.mt_bench_single_turn\\\"\\\n",
       "          \\n    ]\\n}\"\n",
       "      template:\n",
       "        ref: response_assessment.rating.mt_bench_single_turn\n",
       "      format: formats.models.mistral.instruction\n",
       "      metrics:\n",
       "      - ref: llmaaj_metric\n",
       "  custom:\n",
       "    templates:\n",
       "    - name: response_assessment.rating.mt_bench_single_turn\n",
       "      value: \"{\\n    \\\"__type__\\\": \\\"input_output_template\\\",\\n    \\\"instruction\\\"\\\n",
       "        : \\\"Please act as an impartial judge and evaluate the quality of the response\\\n",
       "        \\ provided by an AI assistant to the user question displayed below. Your evaluation\\\n",
       "        \\ should consider factors such as the helpfulness, relevance, accuracy, depth,\\\n",
       "        \\ creativity, and level of detail of the response. Begin your evaluation by\\\n",
       "        \\ providing a short explanation. Be as objective as possible. After providing\\\n",
       "        \\ your explanation, you must rate the response on a scale of 1 to 10 by strictly\\\n",
       "        \\ following this format: \\\\\\\"[[rating]]\\\\\\\", for example: \\\\\\\"Rating: [[5]]\\\\\\\n",
       "        \\\".\\\\n\\\\n\\\",\\n    \\\"input_format\\\": \\\"[Question]\\\\n{question}\\\\n\\\\n[The Start\\\n",
       "        \\ of Assistant's Answer]\\\\n{answer}\\\\n[The End of Assistant's Answer]\\\",\\n\\\n",
       "        \\    \\\"output_format\\\": \\\"[[{rating}]]\\\",\\n    \\\"postprocessors\\\": [\\n   \\\n",
       "        \\     \\\"processors.extract_mt_bench_rating_judgment\\\"\\n    ]\\n}\"\n",
       "    tasks:\n",
       "    - name: response_assessment.rating.single_turn\n",
       "      value: \"{\\n    \\\"__type__\\\": \\\"task\\\",\\n    \\\"input_fields\\\": {\\n        \\\"\\\n",
       "        question\\\": \\\"str\\\",\\n        \\\"answer\\\": \\\"str\\\"\\n    },\\n    \\\"outputs\\\"\\\n",
       "        : {\\n        \\\"rating\\\": \\\"float\\\"\\n    },\\n    \\\"metrics\\\": [\\n        \\\"\\\n",
       "        metrics.spearman\\\"\\n    ]\\n}\"\n",
       "    metrics:\n",
       "    - name: llmaaj_metric\n",
       "      value: \"{\\n    \\\"__type__\\\": \\\"llm_as_judge\\\",\\n    \\\"inference_model\\\": {\\n\\\n",
       "        \\        \\\"__type__\\\": \\\"hf_pipeline_based_inference_engine\\\",\\n        \\\"\\\n",
       "        model_name\\\": \\\"mistralai/Mistral-7B-Instruct-v0.2\\\",\\n        \\\"max_new_tokens\\\"\\\n",
       "        : 256,\\n        \\\"use_fp16\\\": true\\n    },\\n    \\\"template\\\": \\\"templates.response_assessment.rating.mt_bench_single_turn\\\"\\\n",
       "        ,\\n    \\\"task\\\": \\\"rating.single_turn\\\",\\n    \\\"format\\\": \\\"formats.models.mistral.instruction\\\"\\\n",
       "        ,\\n    \\\"main_score\\\": \\\"mistralai_mistral_7b_instruct_v0.2_huggingface_template_mt_bench_single_turn\\\"\\\n",
       "        \\n}\"\n",
       "  logSamples: true\n",
       "  allowOnline: true\n",
       "  allowCodeExecution: true\n",
       "  pod:\n",
       "    container:\n",
       "      env:\n",
       "      - name: HF_TOKEN\n",
       "        value: <HF_TOKEN>\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the complete LLM-as-a-Judge job\n",
    "llm_judge_job = (LMEvalJobBuilder(\"custom-llmaaj-metric\")\n",
    "                 .model(\"hf\")\n",
    "                 .add_model_arg(\"pretrained\", \"google/flan-t5-small\")\n",
    "                 .custom_card(\n",
    "                     card_json=mt_bench_card_json,\n",
    "                     template_ref=\"response_assessment.rating.mt_bench_single_turn\",\n",
    "                     format_str=\"formats.models.mistral.instruction\",\n",
    "                     metrics=[\"llmaaj_metric\"]\n",
    "                 )\n",
    "                 .custom_definitions(\n",
    "                     templates=[mt_bench_template],\n",
    "                     tasks=[rating_task],\n",
    "                     metrics=[llm_judge_metric]\n",
    "                 )\n",
    "                 .log_samples(True)\n",
    "                 .allow_online(True)\n",
    "                 .allow_code_execution(True)\n",
    "                 .hf_token(\"<HF_TOKEN>\")\n",
    "                 .build())\n",
    "\n",
    "print(\"‚úÖ Created complete LLM-as-a-Judge evaluation:\")\n",
    "print(f\"   Job name: {llm_judge_job.metadata.name}\")\n",
    "print(f\"   Model: {llm_judge_job.spec.modelArgs[0].value}\")\n",
    "print(f\"   Custom templates: {len(llm_judge_job.spec.custom.templates)}\")\n",
    "print(f\"   Custom tasks: {len(llm_judge_job.spec.custom.tasks)}\")\n",
    "print(f\"   Custom metrics: {len(llm_judge_job.spec.custom.metrics)}\")\n",
    "print()\n",
    "\n",
    "# Display the full YAML\n",
    "display(Markdown(\"### Complete LLM-as-a-Judge YAML:\"))\n",
    "display(Markdown(f\"```yaml\\n{llm_judge_job.to_yaml()}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploring the Generated Structure\n",
    "\n",
    "Let's examine the structure of our generated CRs to understand what we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing the task card structure:\n",
      "   Card type: task_card\n",
      "   Loader type: load_hf\n",
      "   Dataset path: OfirArviv/mt_bench_single_score_gpt4_judgement\n",
      "   Preprocessing steps: 8\n",
      "\n",
      "üìã Preprocessing step types:\n",
      "   1. rename_splits\n",
      "   2. filter_by_condition\n",
      "   3. filter_by_condition\n",
      "   4. rename\n",
      "   5. literal_eval\n",
      "   6. copy\n",
      "   7. literal_eval\n",
      "   8. copy\n",
      "\n",
      "üîß Custom definitions:\n",
      "   Template type: input_output_template\n",
      "   Task type: task\n",
      "   Metric type: llm_as_judge\n",
      "   Judge model: mistralai/Mistral-7B-Instruct-v0.2\n"
     ]
    }
   ],
   "source": [
    "# Examine the card.custom field structure\n",
    "recipe = llm_judge_job.spec.taskList.taskRecipes[0]\n",
    "card_custom = json.loads(recipe.card.custom)\n",
    "\n",
    "print(\"üîç Analyzing the task card structure:\")\n",
    "print(f\"   Card type: {card_custom['__type__']}\")\n",
    "print(f\"   Loader type: {card_custom['loader']['__type__']}\")\n",
    "print(f\"   Dataset path: {card_custom['loader']['path']}\")\n",
    "print(f\"   Preprocessing steps: {len(card_custom['preprocess_steps'])}\")\n",
    "print()\n",
    "\n",
    "print(\"üìã Preprocessing step types:\")\n",
    "for i, step in enumerate(card_custom['preprocess_steps']):\n",
    "    print(f\"   {i+1}. {step['__type__']}\")\n",
    "print()\n",
    "\n",
    "# Examine custom definitions\n",
    "print(\"üîß Custom definitions:\")\n",
    "template_config = json.loads(llm_judge_job.spec.custom.templates[0].value)\n",
    "task_config = json.loads(llm_judge_job.spec.custom.tasks[0].value)\n",
    "metric_config = json.loads(llm_judge_job.spec.custom.metrics[0].value)\n",
    "\n",
    "print(f\"   Template type: {template_config['__type__']}\")\n",
    "print(f\"   Task type: {task_config['__type__']}\")\n",
    "print(f\"   Metric type: {metric_config['__type__']}\")\n",
    "print(f\"   Judge model: {metric_config['inference_model']['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Different Judge Models\n",
    "\n",
    "Demonstrate how easy it is to swap judge models and create variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created job 'eval-llama_2_7b_chat_hf' with judge: meta-llama/Llama-2-7b-chat-hf\n",
      "‚úÖ Created job 'eval-dialogpt_large' with judge: microsoft/DialoGPT-large\n",
      "‚úÖ Created job 'eval-flan_t5_xl' with judge: google/flan-t5-xl\n",
      "\n",
      "üéØ Generated 3 evaluation jobs with different judge models!\n"
     ]
    }
   ],
   "source": [
    "# Create variations with different judge models\n",
    "judge_models = [\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"microsoft/DialoGPT-large\",\n",
    "    \"google/flan-t5-xl\"\n",
    "]\n",
    "\n",
    "jobs = []\n",
    "for model in judge_models:\n",
    "    # Create custom metric for this judge\n",
    "    custom_metric = create_llm_as_judge_metric(model)\n",
    "    \n",
    "    # Create job with this judge\n",
    "    job = (LMEvalJobBuilder(f\"eval-{model.split('/')[-1].lower().replace('-', '_')}\")\n",
    "           .namespace(\"experiments\")\n",
    "           .pretrained_model(\"google/flan-t5-base\")\n",
    "           .custom_card(\n",
    "               card_json=mt_bench_card_json,\n",
    "               template_ref=\"response_assessment.rating.mt_bench_single_turn\",\n",
    "               format_str=\"formats.models.llama.chat\" if \"llama\" in model.lower() else \"formats.models.mistral.instruction\",\n",
    "               metrics=[\"llmaaj_metric\"]\n",
    "           )\n",
    "           .custom_definitions(\n",
    "               templates=[mt_bench_template],\n",
    "               tasks=[rating_task],\n",
    "               metrics=[custom_metric]\n",
    "           )\n",
    "           .build())\n",
    "    \n",
    "    jobs.append(job)\n",
    "    \n",
    "    print(f\"‚úÖ Created job '{job.metadata.name}' with judge: {model}\")\n",
    "\n",
    "print(f\"\\nüéØ Generated {len(jobs)} evaluation jobs with different judge models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deployment Ready Output\n",
    "\n",
    "Generate deployment-ready YAML files that can be applied to a Kubernetes cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Production-ready LMEvalJob:\n",
      "   Name: production-evaluation\n",
      "   Namespace: trustyai-production\n",
      "   Security: Online=False, Code=False\n",
      "   Sample limit: 1000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Deployment Commands:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "```bash\n",
       "# Save to file\n",
       "kubectl apply -f production_job.yaml\n",
       "\n",
       "# Or apply directly\n",
       "cat <<EOF | kubectl apply -f -\n",
       "# [YAML content would go here]\n",
       "EOF\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Production YAML:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "apiVersion: trustyai.opendatahub.io/v1alpha1\n",
       "kind: LMEvalJob\n",
       "metadata:\n",
       "  name: production-evaluation\n",
       "  namespace: trustyai-production\n",
       "spec:\n",
       "  model: hf\n",
       "  modelArgs:\n",
       "  - name: pretrained\n",
       "    value: microsoft/DialoGPT-large\n",
       "  taskList:\n",
       "    taskNames:\n",
       "    - hellaswag\n",
       "    - arc_challenge\n",
       "    - truthfulqa_mc\n",
       "  logSamples: true\n",
       "  allowOnline: false\n",
       "  allowCodeExecution: false\n",
       "  limit: '1000'\n",
       "  pod:\n",
       "    container:\n",
       "      env:\n",
       "      - name: HF_TOKEN\n",
       "        value: ${HF_TOKEN}\n",
       "      - name: CUDA_VISIBLE_DEVICES\n",
       "        value: '0'\n",
       "      - name: TRANSFORMERS_CACHE\n",
       "        value: /cache\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a production-ready job with proper configuration\n",
    "production_job = (LMEvalJobBuilder(\"production-evaluation\")\n",
    "                  .namespace(\"trustyai-production\")\n",
    "                  .pretrained_model(\"microsoft/DialoGPT-large\")\n",
    "                  .task_names([\"hellaswag\", \"arc_challenge\", \"truthfulqa_mc\"])\n",
    "                  .limit(1000)\n",
    "                  .log_samples(True)\n",
    "                  .allow_online(False)  # Secure: no online access\n",
    "                  .allow_code_execution(False)  # Secure: no code execution\n",
    "                  .env_var(\"HF_TOKEN\", \"${HF_TOKEN}\")  # Use secret reference\n",
    "                  .env_var(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "                  .env_var(\"TRANSFORMERS_CACHE\", \"/cache\")\n",
    "                  .build())\n",
    "\n",
    "print(\"üöÄ Production-ready LMEvalJob:\")\n",
    "print(f\"   Name: {production_job.metadata.name}\")\n",
    "print(f\"   Namespace: {production_job.metadata.namespace}\")\n",
    "print(f\"   Security: Online={production_job.spec.allowOnline}, Code={production_job.spec.allowCodeExecution}\")\n",
    "print(f\"   Sample limit: {production_job.spec.limit}\")\n",
    "print()\n",
    "\n",
    "# Display deployment command\n",
    "display(Markdown(\"### Deployment Commands:\"))\n",
    "display(Markdown(\"\"\"\n",
    "```bash\n",
    "# Save to file\n",
    "kubectl apply -f production_job.yaml\n",
    "\n",
    "# Or apply directly\n",
    "cat <<EOF | kubectl apply -f -\n",
    "# [YAML content would go here]\n",
    "EOF\n",
    "```\n",
    "\"\"\"))\n",
    "\n",
    "# Show the final YAML\n",
    "display(Markdown(\"### Production YAML:\"))\n",
    "display(Markdown(f\"```yaml\\n{production_job.to_yaml()}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Deploying LMEvalJob to namespace: test\n",
      "[DEBUG] API Group: trustyai.opendatahub.io, Version: v1alpha1\n",
      "[DEBUG] Resource metadata: {'name': 'simple-evaluation', 'namespace': 'test'}\n",
      "[DEBUG] Successfully created LMEvalJob 'simple-evaluation' in namespace 'test'\n",
      "‚úÖ Successfully submitted: LMEvalJob(name='simple-evaluation', namespace='test')\n",
      "   Resource name: simple-evaluation\n",
      "   Namespace: test\n",
      "   Kind: LMEvalJob\n",
      "\n",
      "üìã Available management methods:\n",
      "   - submitted_job.get_status()      # Get current status\n",
      "   - submitted_job.get_logs()        # Get pod logs\n",
      "   - submitted_job.wait_for_completion()  # Wait for completion\n",
      "   - submitted_job.is_running()      # Check if running\n",
      "   - submitted_job.is_completed()    # Check if completed\n",
      "   - submitted_job.is_failed()       # Check if failed\n",
      "   - submitted_job.delete()          # Delete the resource\n",
      "\n",
      "üîç Example usage (would require actual cluster):\n",
      "   status = submitted_job.get_status()\n",
      "   if submitted_job.is_running():\n",
      "       print('Job is currently running...')\n"
     ]
    }
   ],
   "source": [
    "# Import the new Kubernetes client\n",
    "from trustyai.core.trustyai_kubernetes_client import TrustyAIKubernetesClient\n",
    "\n",
    "# Create a client (uses default kubeconfig)\n",
    "client = TrustyAIKubernetesClient()\n",
    "\n",
    "# Submit a job and get a resource handle\n",
    "submitted_job = client.submit(simple_job)\n",
    "\n",
    "if submitted_job:\n",
    "    print(f\"‚úÖ Successfully submitted: {submitted_job}\")\n",
    "    print(f\"   Resource name: {submitted_job.name}\")\n",
    "    print(f\"   Namespace: {submitted_job.namespace}\")\n",
    "    print(f\"   Kind: {submitted_job.kind}\")\n",
    "    print()\n",
    "    \n",
    "    # Demonstrate resource management methods\n",
    "    print(\"üìã Available management methods:\")\n",
    "    print(\"   - submitted_job.get_status()      # Get current status\")\n",
    "    print(\"   - submitted_job.get_logs()        # Get pod logs\")\n",
    "    print(\"   - submitted_job.wait_for_completion()  # Wait for completion\")\n",
    "    print(\"   - submitted_job.is_running()      # Check if running\")\n",
    "    print(\"   - submitted_job.is_completed()    # Check if completed\")\n",
    "    print(\"   - submitted_job.is_failed()       # Check if failed\")\n",
    "    print(\"   - submitted_job.delete()          # Delete the resource\")\n",
    "    print()\n",
    "    \n",
    "    # Example: Check if the job would be running (this would fail in demo environment)\n",
    "    print(\"üîç Example usage (would require actual cluster):\")\n",
    "    print(\"   status = submitted_job.get_status()\")\n",
    "    print(\"   if submitted_job.is_running():\")\n",
    "    print(\"       print('Job is currently running...')\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Failed to submit job (likely no cluster available in demo environment)\")\n",
    "    print(\"üí° The client provides these methods for actual cluster deployment:\")\n",
    "    print(\"   - client.submit(lmeval_job)       # Submit and get resource handle\")\n",
    "    print(\"   - client.list_resources()         # List all resources\")\n",
    "    print(\"   - client.get_resource(name)       # Get handle to existing resource\")\n",
    "    print(\"   - client.generate_yaml(job)       # Generate YAML for kubectl\")\n",
    "    print(\"   - client.save_yaml_to_file(job, path)  # Save YAML to file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Kubernetes Client for TrustyAI Resources\n",
    "\n",
    "The TrustyAI SDK now includes a dedicated Kubernetes client for submitting and managing TrustyAI resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üÜï **New Features:**\n",
    "\n",
    "- **`LMEvalJobBuilder.simple()`**: Static method for creating simple jobs with optional limit parameter (replaces deprecated `create_simple_lmeval_job`)\n",
    "- **`TrustyAIKubernetesClient`**: Dedicated client for TrustyAI resources\n",
    "- **`SubmittedResource`**: Handle returned by `client.submit()` with methods for:\n",
    "  - Status checking (`get_status()`, `is_running()`, `is_completed()`, `is_failed()`)\n",
    "  - Log retrieval (`get_logs()`)\n",
    "  - Resource management (`delete()`, `wait_for_completion()`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
