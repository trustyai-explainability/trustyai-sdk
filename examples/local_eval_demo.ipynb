{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# TrustyAI Evaluation Demo\n",
        "\n",
        "This notebook demonstrates how to use the TrustyAI SDK's evaluation functionality to evaluate language models using the LM Evaluation Harness both locally and on Kubernetes.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Make sure you have installed TrustyAI with evaluation support:\n",
        "\n",
        "```bash\n",
        "pip install .[eval]\n",
        "```\n",
        "\n",
        "Or for all features:\n",
        "\n",
        "```bash\n",
        "pip install .[all]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Basic Setup and Imports\n",
        "\n",
        "First, let's import the necessary modules and check what evaluation providers are available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from trustyai import Providers\n",
        "from trustyai.core import DeploymentMode\n",
        "from trustyai.core.eval import EvaluationProviderConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== TrustyAI Providers Class ===\n",
            "\n",
            "Available provider types:\n",
            "  - bias_detection\n",
            "  - eval\n",
            "  - evaluation\n",
            "  - explainability\n",
            "\n",
            "Available evaluation providers:\n",
            "  - LMEvalProvider\n",
            "  - Lm-evalProvider\n",
            "\n",
            "We'll use: Providers.eval.LMEvalProvider\n",
            "The deployment mode in the config will determine whether it runs locally or on Kubernetes.\n"
          ]
        }
      ],
      "source": [
        "print(\"=== TrustyAI Providers Class ===\")\n",
        "print(\"\\nAvailable provider types:\")\n",
        "for provider_type in dir(Providers):\n",
        "    if not provider_type.startswith('_'):\n",
        "        print(f\"  - {provider_type}\")\n",
        "\n",
        "print(f\"\\nAvailable evaluation providers:\")\n",
        "for provider_name in dir(Providers.eval):\n",
        "    if not provider_name.startswith('_'):\n",
        "        print(f\"  - {provider_name}\")\n",
        "\n",
        "print(f\"\\nWe'll use: Providers.eval.LMEvalProvider\")\n",
        "print(\"The deployment mode in the config will determine whether it runs locally or on Kubernetes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Initialise the Evaluation Provider\n",
        "\n",
        "Now let's create and initialise the evaluation provider using the new organised Providers class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/rui/Sync/code/rh/trusty/trustyai-sdk/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ“ Evaluation provider initialised successfully!\n",
            "Provider class: LMEvalProvider\n",
            "Provider type: eval\n",
            "Supported deployment modes: ['local', 'kubernetes']\n"
          ]
        }
      ],
      "source": [
        "# Create the evaluation provider using the new Providers class\n",
        "provider = Providers.eval.LMEvalProvider()\n",
        "\n",
        "# Initialise the provider (this will check if lm-eval is available)\n",
        "try:\n",
        "    provider.initialize()\n",
        "    print(\"\\nâœ“ Evaluation provider initialised successfully!\")\n",
        "    print(f\"Provider class: {provider.__class__.__name__}\")\n",
        "    print(f\"Provider type: {provider.get_provider_type()}\")\n",
        "    print(f\"Supported deployment modes: {[mode.value for mode in provider.supported_deployment_modes]}\")\n",
        "except ImportError as e:\n",
        "    print(f\"\\nâœ— Error initialising provider: {e}\")\n",
        "    print(\"Please install evaluation dependencies: pip install .[eval]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Basic Evaluation Example\n",
        "\n",
        "Let's run a basic evaluation using a small model and a simple task. We'll use google/flan-t5-base (a small model) and the arc_easy task for demonstration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration created:\n",
            "  Model: google/flan-t5-base\n",
            "  Tasks: ['arc_easy']\n",
            "  Metrics: ['acc', 'acc_norm']\n",
            "  Device: cpu\n",
            "  Limit: 5 examples\n",
            "  Batch size: 1\n",
            "  Few-shot examples: 0\n"
          ]
        }
      ],
      "source": [
        "# Create evaluation configuration\n",
        "config = EvaluationProviderConfig(\n",
        "    evaluation_name=\"arc_easy\",\n",
        "    model=\"google/flan-t5-base\",  # Small model for quick evaluation\n",
        "    tasks=[\"arc_easy\"],  # Common sense reasoning task\n",
        "    limit=5,  # Limit to 5 examples for quick demonstration\n",
        "    metrics=[\"acc\", \"acc_norm\"],  # Accuracy metrics\n",
        "    device=\"cpu\",  # Use CPU to avoid GPU requirements\n",
        "    deployment_mode=DeploymentMode.LOCAL,\n",
        "    batch_size=1,  # Small batch size for stability\n",
        "    num_fewshot=0  # Zero-shot evaluation\n",
        ")\n",
        "\n",
        "print(\"Configuration created:\")\n",
        "print(f\"  Model: {config.model}\")\n",
        "print(f\"  Tasks: {config.tasks}\")\n",
        "print(f\"  Metrics: {config.metrics}\")\n",
        "print(f\"  Device: {config.device}\")\n",
        "print(f\"  Limit: {config.limit} examples\")\n",
        "print(f\"  Batch size: {config.get_param('batch_size')}\")\n",
        "print(f\"  Few-shot examples: {config.get_param('num_fewshot')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-17:21:10:47,459 INFO     [lm_eval.models.huggingface:136] Using device 'cpu'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running evaluation...\n",
            "[DEBUG - _parse_args_to_config] Args=1: has namespace? False\n",
            "Using device: cpu for model evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-17:21:10:47,827 INFO     [lm_eval.models.huggingface:376] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cpu'}\n",
            "2025-06-17:21:10:53,121 INFO     [lm_eval.evaluator:177] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-06-17:21:10:53,122 INFO     [lm_eval.evaluator:230] Using pre-initialized model\n",
            "2025-06-17:21:10:55,849 WARNING  [lm_eval.evaluator:283] Overwriting default num_fewshot of arc_easy from None to 0\n",
            "2025-06-17:21:10:55,850 INFO     [lm_eval.api.task:420] Building contexts for arc_easy on rank 0...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 2473.06it/s]\n",
            "2025-06-17:21:10:55,853 INFO     [lm_eval.evaluator:525] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 21.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ“ Evaluation completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Run the evaluation\n",
        "print(\"Running evaluation...\")\n",
        "try:\n",
        "    results = provider.evaluate(config)\n",
        "    print(\"\\nâœ“ Evaluation completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nâœ— Evaluation failed: {e}\")\n",
        "    results = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Local vs Kubernetes Comparison\n",
        "\n",
        "Compare the same configuration running locally vs on Kubernetes. This demonstrates how the same provider handles both deployment modes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”„ RUNNING LOCAL vs KUBERNETES COMPARISON\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Shared configuration for both deployments\n",
        "shared_config = {\n",
        "    \"evaluation_name\": \"comparison_demo\",\n",
        "    \"model\": \"google/flan-t5-base\",\n",
        "    \"tasks\": [\"arc_easy\"],\n",
        "    \"limit\": 3,  # Small limit for quick comparison\n",
        "    \"metrics\": [\"acc\", \"acc_norm\"],\n",
        "    \"batch_size\": 1,\n",
        "    \"num_fewshot\": 0\n",
        "}\n",
        "\n",
        "# Configuration for LOCAL deployment\n",
        "local_config = EvaluationProviderConfig(\n",
        "    **shared_config,\n",
        "    deployment_mode=DeploymentMode.LOCAL,\n",
        "    device=\"cpu\"\n",
        ")\n",
        "\n",
        "# Configuration for KUBERNETES deployment\n",
        "kubernetes_config = EvaluationProviderConfig(\n",
        "    **shared_config,\n",
        "    deployment_mode=DeploymentMode.KUBERNETES,\n",
        "    namespace=\"test\",\n",
        "    deploy=True,\n",
        "    wait_for_completion=True,\n",
        "    timeout=300\n",
        ")\n",
        "\n",
        "print(\"ðŸ”„ RUNNING LOCAL vs KUBERNETES COMPARISON\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-17:21:11:21,621 INFO     [lm_eval.models.huggingface:136] Using device 'cpu'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“ LOCAL Configuration:\n",
            "  Deployment mode: local\n",
            "  Device: cpu\n",
            "\n",
            "Running local evaluation...\n",
            "[DEBUG - _parse_args_to_config] Args=1: has namespace? False\n",
            "Using device: cpu for model evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-17:21:11:21,891 INFO     [lm_eval.models.huggingface:376] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cpu'}\n",
            "2025-06-17:21:11:27,198 INFO     [lm_eval.evaluator:177] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
            "2025-06-17:21:11:27,198 INFO     [lm_eval.evaluator:230] Using pre-initialized model\n",
            "2025-06-17:21:11:29,205 WARNING  [lm_eval.evaluator:283] Overwriting default num_fewshot of arc_easy from None to 0\n",
            "2025-06-17:21:11:29,206 INFO     [lm_eval.api.task:420] Building contexts for arc_easy on rank 0...\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 2394.01it/s]\n",
            "2025-06-17:21:11:29,208 INFO     [lm_eval.evaluator:525] Running loglikelihood requests\n",
            "Running loglikelihood requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:00<00:00, 20.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Local evaluation completed!\n"
          ]
        }
      ],
      "source": [
        "# Run local evaluation\n",
        "print(f\"ðŸ“ LOCAL Configuration:\")\n",
        "print(f\"  Deployment mode: {local_config.deployment_mode.value}\")\n",
        "print(f\"  Device: {local_config.device}\")\n",
        "\n",
        "print(\"\\nRunning local evaluation...\")\n",
        "local_results = None\n",
        "try:\n",
        "    local_results = provider.evaluate(local_config)\n",
        "    print(\"âœ“ Local evaluation completed!\")\n",
        "except Exception as e:\n",
        "    print(f\"âœ— Local evaluation failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ KUBERNETES Configuration:\n",
            "  Deployment mode: kubernetes\n",
            "  Namespace: test\n",
            "  Deploy: True\n",
            "  Wait for completion: True\n",
            "\n",
            "Running Kubernetes evaluation...\n",
            "[DEBUG - _parse_args_to_config] Args=1: has namespace? True\n",
            "[DEBUG - _parse_args_to_config] Namespace value: test\n",
            "[DEBUG - _evaluate_kubernetes_async] Config keys: ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'additional_params', 'deployment_mode', 'device', 'evaluation_name', 'get_param', 'limit', 'metrics', 'model', 'tasks']\n",
            "[DEBUG - _evaluate_kubernetes_async] Config namespace: test\n",
            "[DEBUG] Using namespace for CR: test\n",
            "[DEBUG] Setting limit in config as string: 3\n",
            "[DEBUG] Setting namespace in LMEvalJob resource: test\n",
            "[DEBUG] Setting limit as string: 3\n",
            "[DEBUG] Deploying LMEvalJob to namespace: test\n",
            "[DEBUG] API Group: trustyai.opendatahub.io, Version: v1alpha1\n",
            "[DEBUG] Resource metadata: {'name': 'evaljob-033ed6e7', 'namespace': 'test'}\n",
            "[DEBUG] Successfully created LMEvalJob 'evaljob-033ed6e7' in namespace 'test'\n",
            "[DEBUG] Successfully deployed LMEvalJob: evaljob-033ed6e7\n",
            "[DEBUG] Waiting for completion of job: evaljob-033ed6e7\n",
            "â³ Waiting for job evaljob-033ed6e7 to complete...\n",
            "[DEBUG] Job evaljob-033ed6e7 state: scheduled\n",
            "ðŸ”„ Job evaljob-033ed6e7 still running (state: scheduled)...\n",
            "[DEBUG] Job evaljob-033ed6e7 state: running\n",
            "ðŸ”„ Job evaljob-033ed6e7 still running (state: running)...\n",
            "[DEBUG] Job evaljob-033ed6e7 state: running\n",
            "ðŸ”„ Job evaljob-033ed6e7 still running (state: running)...\n",
            "[DEBUG] Job evaljob-033ed6e7 state: complete\n",
            "âœ… Job evaljob-033ed6e7 completed successfully!\n",
            "âœ“ Kubernetes evaluation completed!\n"
          ]
        }
      ],
      "source": [
        "# Helper function for Kubernetes evaluation\n",
        "async def run_kubernetes_evaluation(provider, config):\n",
        "    \"\"\"Run Kubernetes evaluation asynchronously.\"\"\"\n",
        "    try:\n",
        "        results = await provider.evaluate(config)\n",
        "        print(\"âœ“ Kubernetes evaluation completed!\")\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— Kubernetes evaluation failed: {e}\")\n",
        "        return {\"status\": \"failed\", \"error\": str(e)}\n",
        "\n",
        "# Run Kubernetes evaluation (async)\n",
        "print(f\"ðŸš€ KUBERNETES Configuration:\")\n",
        "print(f\"  Deployment mode: {kubernetes_config.deployment_mode.value}\")\n",
        "print(f\"  Namespace: {kubernetes_config.get_param('namespace')}\")\n",
        "print(f\"  Deploy: {kubernetes_config.get_param('deploy')}\")\n",
        "print(f\"  Wait for completion: {kubernetes_config.get_param('wait_for_completion')}\")\n",
        "\n",
        "print(\"\\nRunning Kubernetes evaluation...\")\n",
        "kubernetes_results = await run_kubernetes_evaluation(provider, kubernetes_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” RESULTS COMPARISON\n",
            "==============================\n",
            "\n",
            "ðŸ“Š LOCAL RESULTS:\n",
            "--------------------\n",
            "âœ… Task: arc_easy\n",
            "   acc,none: 0.6667\n",
            "   acc_stderr,none: 0.3333\n",
            "   acc_norm,none: 0.6667\n",
            "   acc_norm_stderr,none: 0.3333\n",
            "\n",
            "ðŸš€ KUBERNETES RESULTS:\n",
            "--------------------\n",
            "âœ… Task: arc_easy\n",
            "   acc,none: 0.6667\n",
            "   acc_stderr,none: 0.3333\n",
            "   acc_norm,none: 0.6667\n",
            "   acc_norm_stderr,none: 0.3333\n",
            "\n",
            "ðŸŽ‰ Demo completed!\n"
          ]
        }
      ],
      "source": [
        "# Compare results\n",
        "print(\"ðŸ” RESULTS COMPARISON\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "print(\"\\nðŸ“Š LOCAL RESULTS:\")\n",
        "print(\"-\" * 20)\n",
        "if local_results and \"results\" in local_results:\n",
        "    for task_name, task_results in local_results[\"results\"].items():\n",
        "        print(f\"âœ… Task: {task_name}\")\n",
        "        for metric, value in task_results.items():\n",
        "            if isinstance(value, (int, float)):\n",
        "                print(f\"   {metric}: {value:.4f}\")\n",
        "else:\n",
        "    print(\"âŒ No local results available\")\n",
        "\n",
        "print(\"\\nðŸš€ KUBERNETES RESULTS:\")\n",
        "print(\"-\" * 20)  \n",
        "if kubernetes_results and \"results\" in kubernetes_results:\n",
        "    for task_name, task_results in kubernetes_results[\"results\"].items():\n",
        "        print(f\"âœ… Task: {task_name}\")\n",
        "        for metric, value in task_results.items():\n",
        "            if isinstance(value, (int, float)):\n",
        "                print(f\"   {metric}: {value:.4f}\")\n",
        "else:\n",
        "    print(\"âŒ No Kubernetes results available\")\n",
        "    if kubernetes_results:\n",
        "        print(f\"Status: {kubernetes_results.get('status', 'unknown')}\")\n",
        "        print(f\"Message: {kubernetes_results.get('message', 'no message')}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ Demo completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
